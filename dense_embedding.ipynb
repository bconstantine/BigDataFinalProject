{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format1_path = \"data_format1\"\n",
    "data_format2_path = \"data_format2\"\n",
    "train_format1_path = os.path.join(data_format1_path, \"train_format1.csv\")\n",
    "train_format2_path = os.path.join(data_format2_path, \"train_format2.csv\")\n",
    "test_format1_path = os.path.join(data_format1_path, \"test_format1.csv\")\n",
    "test_format2_path = os.path.join(data_format2_path, \"test_format2.csv\")\n",
    "user_info_format1_path = os.path.join(data_format1_path, \"user_info_format1.csv\") \n",
    "user_log_format1_path = os.path.join(data_format1_path, \"user_log_format1.csv\", ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_format1 = pd.read_csv(train_format1_path)\n",
    "#train_format2 = pd.read_csv(train_format2_path)\n",
    "test_format1 = pd.read_csv(test_format1_path)\n",
    "#test_format2 = pd.read_csv(test_format2_path)\n",
    "user_info_format1 = pd.read_csv(user_info_format1_path)\n",
    "user_log_format1 = pd.read_csv(user_log_format1_path, dtype={'time_stamp': str}).rename(columns={\"seller_id\": \"merchant_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User id amount 424170\n",
      "Brand id amount 8444\n",
      "Merchant id amount 1994\n",
      "Cat id amount 1658\n"
     ]
    }
   ],
   "source": [
    "print(\"User id amount\", len(set(train_format1[\"user_id\"].unique()).union(set(test_format1[\"user_id\"].unique()))))\n",
    "print(\"Brand id amount\", len(set(user_log_format1[\"brand_id\"].unique())))\n",
    "print(\"Merchant id amount\", len(set(train_format1[\"merchant_id\"].unique()).union(set(test_format1[\"merchant_id\"].unique()))))\n",
    "print(\"Cat id amount\", len(set(user_log_format1[\"cat_id\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_format1: 260864\n",
      "len test_format1: 261477\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_format1:\", len(train_format1))\n",
    "#print(\"len train_format2:\", len(train_format2))\n",
    "print(\"len test_format1:\", len(test_format1))\n",
    "#print(\"len test_format2:\", len(test_format2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54925330\n",
      "10582633\n"
     ]
    }
   ],
   "source": [
    "print(len(user_log_format1))\n",
    "#only filter the log with time_stamp 11.11\n",
    "user_log_format1 = user_log_format1[user_log_format1[\"time_stamp\"] == \"1111\"]\n",
    "print(len(user_log_format1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id  item_id  cat_id  merchant_id  brand_id time_stamp  action_type\n",
      "171   328862   406349    1280         2700    5476.0       1111            0\n",
      "172   328862   406349    1280         2700    5476.0       1111            0\n",
      "173   328862   807126    1181         1963    6109.0       1111            0\n",
      "174   328862   406349    1280         2700    5476.0       1111            2\n",
      "175   328862   406349    1280         2700    5476.0       1111            0\n"
     ]
    }
   ],
   "source": [
    "print(user_log_format1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_format1 = user_log_format1.drop(columns=[\"action_type\", \"time_stamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_log_info1 columns: Index(['user_id', 'item_id', 'cat_id', 'merchant_id', 'brand_id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"user_log_info1 columns: \" + str(user_log_format1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  merchant_id  label\n",
      "0    34176         3906      0\n",
      "1    34176          121      0\n",
      "2    34176         4356      1\n",
      "3    34176         2217      0\n",
      "4   230784         4818      0\n",
      "============================================\n",
      "             user_id    merchant_id          label\n",
      "count  260864.000000  260864.000000  260864.000000\n",
      "mean   211889.321420    2540.292363       0.061151\n",
      "std    122399.488027    1451.207514       0.239607\n",
      "min         1.000000       2.000000       0.000000\n",
      "25%    105488.000000    1340.000000       0.000000\n",
      "50%    211927.500000    2482.000000       0.000000\n",
      "75%    317670.500000    3898.000000       0.000000\n",
      "max    424170.000000    4993.000000       1.000000\n",
      "============================================\n",
      "user_id        0\n",
      "merchant_id    0\n",
      "label          0\n",
      "dtype: int64\n",
      "user_id        0.0\n",
      "merchant_id    0.0\n",
      "label          0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_format1.head())\n",
    "print(\"============================================\")\n",
    "print(train_format1.describe())\n",
    "print(\"============================================\")\n",
    "#check for nan\n",
    "print(train_format1.isnull().sum())\n",
    "#nan ratio\n",
    "print(train_format1.isnull().sum()/len(train_format1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_format2.head())\n",
    "# print(\"============================================\")\n",
    "# print(train_format2.describe())\n",
    "# print(\"============================================\")\n",
    "# #check for nan\n",
    "# print(train_format2.isnull().sum())\n",
    "# #nan ratio\n",
    "# print(train_format2.isnull().sum()/len(train_format2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  merchant_id  prob\n",
      "0   163968         4605   NaN\n",
      "1   360576         1581   NaN\n",
      "2    98688         1964   NaN\n",
      "3    98688         3645   NaN\n",
      "4   295296         3361   NaN\n",
      "============================================\n",
      "============================================\n",
      "user_id             0\n",
      "merchant_id         0\n",
      "prob           261477\n",
      "dtype: int64\n",
      "user_id        0.0\n",
      "merchant_id    0.0\n",
      "prob           1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(test_format1.head())\n",
    "print(\"============================================\")\n",
    "test_format1.describe()\n",
    "print(\"============================================\")\n",
    "print(test_format1.isnull().sum())\n",
    "print(test_format1.isnull().sum()/len(test_format1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_format2.head())\n",
    "# print(\"============================================\")\n",
    "# test_format2.describe()\n",
    "# print(\"============================================\")\n",
    "# print(test_format2.isnull().sum())\n",
    "# print(test_format2.isnull().sum()/len(test_format2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  age_range  gender\n",
      "0   376517        6.0     1.0\n",
      "1   234512        5.0     0.0\n",
      "2   344532        5.0     0.0\n",
      "3   186135        5.0     0.0\n",
      "4    30230        5.0     0.0\n",
      "============================================\n",
      "             user_id      age_range         gender\n",
      "count  424170.000000  421953.000000  417734.000000\n",
      "mean   212085.500000       2.930262       0.341179\n",
      "std    122447.476179       1.942978       0.524112\n",
      "min         1.000000       0.000000       0.000000\n",
      "25%    106043.250000       2.000000       0.000000\n",
      "50%    212085.500000       3.000000       0.000000\n",
      "75%    318127.750000       4.000000       1.000000\n",
      "max    424170.000000       8.000000       2.000000\n",
      "============================================\n",
      "user_id         0\n",
      "age_range    2217\n",
      "gender       6436\n",
      "dtype: int64\n",
      "user_id      0.000000\n",
      "age_range    0.005227\n",
      "gender       0.015173\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(user_info_format1.head())\n",
    "print(\"============================================\")\n",
    "print(user_info_format1.describe())\n",
    "print(\"============================================\")\n",
    "print(user_info_format1.isnull().sum())\n",
    "print(user_info_format1.isnull().sum()/len(user_info_format1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id  item_id  cat_id  merchant_id  brand_id\n",
      "171   328862   406349    1280         2700    5476.0\n",
      "172   328862   406349    1280         2700    5476.0\n",
      "173   328862   807126    1181         1963    6109.0\n",
      "174   328862   406349    1280         2700    5476.0\n",
      "175   328862   406349    1280         2700    5476.0\n",
      "============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            user_id       item_id        cat_id   merchant_id      brand_id\n",
      "count  1.058263e+07  1.058263e+07  1.058263e+07  1.058263e+07  1.056487e+07\n",
      "mean   2.123082e+05  5.477336e+05  8.938849e+02  2.408522e+03  4.079078e+03\n",
      "std    1.222255e+05  3.246887e+05  4.371969e+02  1.512776e+03  2.422199e+03\n",
      "min    1.000000e+00  2.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "25%    1.067360e+05  2.608150e+05  6.020000e+02  1.087000e+03  1.866000e+03\n",
      "50%    2.125260e+05  5.544080e+05  9.460000e+02  2.387000e+03  3.969000e+03\n",
      "75%    3.179590e+05  8.225780e+05  1.238000e+03  3.760000e+03  6.143000e+03\n",
      "max    4.241700e+05  1.113166e+06  1.671000e+03  4.995000e+03  8.477000e+03\n",
      "============================================\n",
      "user_id            0\n",
      "item_id            0\n",
      "cat_id             0\n",
      "merchant_id        0\n",
      "brand_id       17767\n",
      "dtype: int64\n",
      "user_id        0.000000\n",
      "item_id        0.000000\n",
      "cat_id         0.000000\n",
      "merchant_id    0.000000\n",
      "brand_id       0.001679\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(user_log_format1.head())\n",
    "print(\"============================================\")\n",
    "print(user_log_format1.describe())\n",
    "print(\"============================================\")\n",
    "print(user_log_format1.isnull().sum())\n",
    "print(user_log_format1.isnull().sum()/len(user_log_format1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [user_id, merchant_id]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [user_id, merchant_id_x, merchant_id_y]\n",
      "Index: []\n",
      "           user_id_x  merchant_id  user_id_y\n",
      "0              34176         3906     325008\n",
      "1              34176         3906     282768\n",
      "2              34176         3906     187542\n",
      "3              34176         3906      97206\n",
      "4              34176         3906     153795\n",
      "...              ...          ...        ...\n",
      "150064189     229247         4140     188243\n",
      "150064190     229247         4140     171110\n",
      "150064191     229247         4140     238697\n",
      "150064192     229247         4140     176240\n",
      "150064193     229247         4140     333428\n",
      "\n",
      "[150064194 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# does there exist user_id and merchant_id pair that appears in test set\n",
    "print(pd.merge(train_format1[[\"user_id\", \"merchant_id\"]].drop_duplicates(), test_format1[[\"user_id\", \"merchant_id\"]].drop_duplicates(), how=\"inner\", on=[\"user_id\", \"merchant_id\"]))\n",
    "print(pd.merge(train_format1[[\"user_id\", \"merchant_id\"]].drop_duplicates(), test_format1[[\"user_id\", \"merchant_id\"]].drop_duplicates(), how=\"inner\", on=[\"user_id\"]))\n",
    "print(pd.merge(train_format1[[\"user_id\", \"merchant_id\"]].drop_duplicates(), test_format1[[\"user_id\", \"merchant_id\"]].drop_duplicates(), how=\"inner\", on=[\"merchant_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id  item_id  cat_id  merchant_id  brand_id\n",
      "4930955   360576   991507    1142         4044      82.0\n",
      "4930956   360576   948181     614         1581    4066.0\n",
      "4930958   360576  1111020     614         1581    4066.0\n",
      "4930959   360576   755622     614         1783    5041.0\n",
      "4930960   360576   294442     614         1581    4066.0\n",
      "4930961   360576   864510     614         1581    4066.0\n",
      "4930962   360576   326679     614         1581    4066.0\n",
      "4930963   360576   720542     748         2582    7208.0\n",
      "4930964   360576   755622     614         1783    5041.0\n",
      "4930967   360576   794107     614         1581    4066.0\n",
      "4930969   360576   864510     614         1581    4066.0\n",
      "4930970   360576   755622     614         1783    5041.0\n",
      "4930971   360576   755622     614         1783    5041.0\n",
      "4930973   360576   982117     632         1581    4066.0\n",
      "4930974   360576    61318     229         1581    4066.0\n",
      "4930975   360576   164776     389         3848    4452.0\n",
      "4930976   360576  1091811    1142         4044      82.0\n",
      "4930977   360576   863626     119         1581    4066.0\n",
      "4930978   360576   613888     748         3311    6548.0\n"
     ]
    }
   ],
   "source": [
    "print(user_log_format1[user_log_format1[\"user_id\"] == 360576])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, merchant_id, prob]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_format1[~test_format1['user_id'].isin(user_log_format1['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test length:  261477\n",
      "User merchant pair in test not appear in log:  261477\n",
      "User in test not appear in log:  0\n",
      "merchant in test not appear in log:  0\n"
     ]
    }
   ],
   "source": [
    "#verify user activity of test users all exist inside the data\n",
    "missing_keys = test_format1[~test_format1[['user_id', 'merchant_id']].isin(user_log_format1[['user_id', 'merchant_id']])]\n",
    "missing_keys_user_only = test_format1[~test_format1['user_id'].isin(user_log_format1['user_id'])]\n",
    "missing_keys_merchant_only = test_format1[~test_format1[ 'merchant_id'].isin(user_log_format1['merchant_id'])]\n",
    "print(\"Original test length: \", len(test_format1))\n",
    "print(\"User merchant pair in test not appear in log: \", len(missing_keys.drop_duplicates()))\n",
    "print(\"User in test not appear in log: \", len(missing_keys_user_only.drop_duplicates()))\n",
    "print(\"merchant in test not appear in log: \", len(missing_keys_merchant_only.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train user id Uniqueness ratio:  0.8129216756624141\n",
      "Test user id Uniqueness ratio:  0.8111918065451263\n",
      "Train merchant id Uniqueness ratio:  0.007639996319921492\n",
      "Test merchant Uniqueness ratio:  0.007622085307694367\n"
     ]
    }
   ],
   "source": [
    "#unique value of user id\n",
    "train_format1['user_id'].nunique()\n",
    "print(\"Train user id Uniqueness ratio: \", train_format1['user_id'].nunique() / len(train_format1['user_id']))\n",
    "print(\"Test user id Uniqueness ratio: \", test_format1['user_id'].nunique() / len(test_format1['user_id']))\n",
    "\n",
    "print(\"Train merchant id Uniqueness ratio: \", train_format1['merchant_id'].nunique() / len(train_format1['merchant_id']))\n",
    "print(\"Test merchant Uniqueness ratio: \", test_format1['merchant_id'].nunique() / len(test_format1['merchant_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of user id and merchant id duplicated in train:  0\n",
      "Amount of user id and merchant id duplicated in test:  0\n"
     ]
    }
   ],
   "source": [
    "#duplicate data in train_format1 between merchant and user id\n",
    "print(\"Amount of user id and merchant id duplicated in train: \", train_format1[['user_id', 'merchant_id']].duplicated().sum())\n",
    "print(\"Amount of user id and merchant id duplicated in test: \",test_format1[['user_id', 'merchant_id']].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if all label -1 in format_2 is all contains in user_log format 1\n",
    "# tmp = test_format2[test_format2[\"label\"] == -1][[\"user_id\", \"merchant_id\"]]\n",
    "# mask = tmp.isin(user_log_format1[[\"user_id\", \"merchant_id\"]]).all(axis=1)\n",
    "# print(\"total of merchant and user in test format 2: \", len(tmp))\n",
    "# print(\"======= Mask ==========\")\n",
    "# print(mask)\n",
    "# print(\"Amount that is contained in format1: \", len(tmp[mask]))\n",
    "# del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  merchant_id  label  item_id  cat_id  brand_id time_stamp  \\\n",
      "0    34176         3906      0   757713     821    6268.0       1110   \n",
      "1    34176         3906      0   757713     821    6268.0       1110   \n",
      "2    34176         3906      0   757713     821    6268.0       1110   \n",
      "3    34176         3906      0   718096    1142    6268.0       1031   \n",
      "4    34176         3906      0   757713     821    6268.0       1031   \n",
      "\n",
      "   action_type  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            3  \n",
      "4            3  \n",
      "====\n",
      "   user_id  merchant_id  prob  item_id  cat_id  brand_id time_stamp  \\\n",
      "0   163968         4605   NaN   772645    1368    7622.0       1111   \n",
      "1   163968         4605   NaN   772645    1368    7622.0       1111   \n",
      "2   360576         1581   NaN   948181     614    4066.0       1111   \n",
      "3   360576         1581   NaN  1111020     614    4066.0       1111   \n",
      "4   360576         1581   NaN   294442     614    4066.0       1111   \n",
      "\n",
      "   action_type  \n",
      "0            2  \n",
      "1            0  \n",
      "2            2  \n",
      "3            2  \n",
      "4            2  \n",
      "====\n",
      "   user_id  merchant_id  label  item_id  cat_id  brand_id time_stamp  \\\n",
      "0    34176         3906      0   757713     821    6268.0       1110   \n",
      "1    34176         3906      0   757713     821    6268.0       1110   \n",
      "2    34176         3906      0   757713     821    6268.0       1110   \n",
      "3    34176         3906      0   718096    1142    6268.0       1031   \n",
      "4    34176         3906      0   757713     821    6268.0       1031   \n",
      "\n",
      "   action_type  age_range  gender  \n",
      "0            0        6.0     0.0  \n",
      "1            0        6.0     0.0  \n",
      "2            0        6.0     0.0  \n",
      "3            3        6.0     0.0  \n",
      "4            3        6.0     0.0  \n",
      "====\n",
      "   user_id  merchant_id  prob  item_id  cat_id  brand_id time_stamp  \\\n",
      "0   163968         4605   NaN   772645    1368    7622.0       1111   \n",
      "1   163968         4605   NaN   772645    1368    7622.0       1111   \n",
      "2   360576         1581   NaN   948181     614    4066.0       1111   \n",
      "3   360576         1581   NaN  1111020     614    4066.0       1111   \n",
      "4   360576         1581   NaN   294442     614    4066.0       1111   \n",
      "\n",
      "   action_type  age_range  gender  \n",
      "0            2        0.0     0.0  \n",
      "1            0        0.0     0.0  \n",
      "2            2        2.0     2.0  \n",
      "3            2        2.0     2.0  \n",
      "4            2        2.0     2.0  \n",
      "====\n"
     ]
    }
   ],
   "source": [
    "#join the tables\n",
    "train_format1 = pd.merge(train_format1, user_log_format1, on=['user_id', 'merchant_id'], how='left')\n",
    "print(train_format1.head())\n",
    "print(\"====\")\n",
    "test_format1 = pd.merge(test_format1, user_log_format1, on=['user_id', 'merchant_id'], how='left')\n",
    "print(test_format1.head())\n",
    "print(\"====\")\n",
    "train_format1 = pd.merge(train_format1, user_info_format1, on='user_id', how='left')\n",
    "print(train_format1.head())\n",
    "print(\"====\")\n",
    "test_format1 = pd.merge(test_format1, user_info_format1, on='user_id', how='left')\n",
    "print(test_format1.head())\n",
    "print(\"====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1653030\n",
      "1650271\n"
     ]
    }
   ],
   "source": [
    "print(len(train_format1))\n",
    "print(len(test_format1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test user id not in train:  212108\n",
      "Test user id not in log 0\n",
      "Test merchant id not in log:  0\n",
      "Test merchant id not in train:  1\n",
      "Category not in train:  86\n",
      "Brand not in train:  3403\n"
     ]
    }
   ],
   "source": [
    "# are all users and merchant in tests appear in trains\n",
    "print(\"Test user id not in train: \", len(set(test_format1['user_id']) - set(train_format1['user_id'])))\n",
    "print(\"Test user id not in log\", len(set(test_format1['user_id']) - set(user_log_format1['user_id'])))\n",
    "print(\"Test merchant id not in log: \", len(set(test_format1['merchant_id']) - set(user_log_format1['merchant_id'])))\n",
    "print(\"Test merchant id not in train: \", len(set(test_format1['merchant_id']) - set(train_format1['merchant_id'])))\n",
    "print(\"Category not in train: \", len(set(test_format1['cat_id']) - set(train_format1['cat_id'])))\n",
    "print(\"Brand not in train: \", len(set(test_format1['brand_id']) - set(train_format1['brand_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424170\n"
     ]
    }
   ],
   "source": [
    "print(len(np.union1d(test_format1['user_id'].unique(), train_format1['user_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212108\n",
      "260864\n"
     ]
    }
   ],
   "source": [
    "print(len(test_format1['user_id'].unique()))\n",
    "print(len(train_format1[['user_id','merchant_id']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erase the original dataframe\n",
    "del user_info_format1\n",
    "del user_log_format1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id  age_range  gender  merchant_id  label  \\\n",
      "0         163968        0.0     0.0         4378   -1.0   \n",
      "1         163968        0.0     0.0         2300   -1.0   \n",
      "2         163968        0.0     0.0         1551   -1.0   \n",
      "3         163968        0.0     0.0         4343   -1.0   \n",
      "4         163968        0.0     0.0         4911   -1.0   \n",
      "...          ...        ...     ...          ...    ...   \n",
      "7027938    32639        0.0     0.0         2550   -1.0   \n",
      "7027939    32639        0.0     0.0         1364   -1.0   \n",
      "7027940    32639        0.0     0.0          503   -1.0   \n",
      "7027941    32639        0.0     0.0         2286   -1.0   \n",
      "7027942    32639        0.0     0.0         1506   -1.0   \n",
      "\n",
      "                                              activity_log  \n",
      "0                                   101206:812:6968:0614:0  \n",
      "1        588758:844:3833:0618:0#71782:844:3833:1111:2#7...  \n",
      "2        312747:243:1954:0627:0#312747:243:1954:0627:0#...  \n",
      "3                                  932390:1612:3201:0628:0  \n",
      "4                                   957657:662:3089:0612:0  \n",
      "...                                                    ...  \n",
      "7027938  380306:177:3545:1028:0#987028:177:3545:1028:0#...  \n",
      "7027939                            927267:1213:6326:0807:0  \n",
      "7027940                              12898:420:5197:0926:3  \n",
      "7027941                            805269:1455:5262:1026:0  \n",
      "7027942                              28017:812:4888:0525:0  \n",
      "\n",
      "[7027943 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_format2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163968</td>\n",
       "      <td>4378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163968</td>\n",
       "      <td>2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163968</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>163968</td>\n",
       "      <td>4343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>163968</td>\n",
       "      <td>4911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027938</th>\n",
       "      <td>32639</td>\n",
       "      <td>2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027939</th>\n",
       "      <td>32639</td>\n",
       "      <td>1364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027940</th>\n",
       "      <td>32639</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027941</th>\n",
       "      <td>32639</td>\n",
       "      <td>2286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027942</th>\n",
       "      <td>32639</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6766466 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  merchant_id\n",
       "0         163968         4378\n",
       "1         163968         2300\n",
       "2         163968         1551\n",
       "3         163968         4343\n",
       "4         163968         4911\n",
       "...          ...          ...\n",
       "7027938    32639         2550\n",
       "7027939    32639         1364\n",
       "7027940    32639          503\n",
       "7027941    32639         2286\n",
       "7027942    32639         1506\n",
       "\n",
       "[6766466 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Matrix Decomposition, with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the embedding matrix of user_id (row), and merchant (column), and embedding vector (latent_size), for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id  merchant_id\n",
      "1        1019           1\n",
      "282488   2193           1\n",
      "282467   2278           1\n",
      "         3958           1\n",
      "         4976           1\n",
      "                       ..\n",
      "140925   2537           1\n",
      "140931   71             1\n",
      "140933   2277           1\n",
      "140934   3835           1\n",
      "424170   4268           1\n",
      "Length: 260864, dtype: int64\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "count_series = train_format1[[\"user_id\", \"merchant_id\"]].value_counts()\n",
    "print(count_series)\n",
    "print(count_series[391188][1102])\n",
    "del count_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User encoder length:  328995\n",
      "Merchant encoder length:  1993\n",
      "Brand encoder length:  4065\n",
      "Cat encoder length:  1463\n",
      "User_encoder.classes_: [     1      2      3 ... 424168 424169 424170]\n",
      "User idx len: 328994, Second idx len: 328995\n",
      "Merchant idx len: 1992, Second idx len: 1993\n",
      "User idx len: 328994, Second idx len: 328995\n",
      "Brand idx index: MultiIndex([(197213,  594),\n",
      "            (209127,  700),\n",
      "            (280693,   41),\n",
      "            (327151, 1690),\n",
      "            (145535, 2586),\n",
      "            (152049, 3379),\n",
      "            (233137, 3491),\n",
      "            (273685,  594),\n",
      "            (161202, 2592),\n",
      "            (163338,  181),\n",
      "            ...\n",
      "            (161914,  742),\n",
      "            (161914,  782),\n",
      "            (161915,  713),\n",
      "            (161915, 1266),\n",
      "            (161915, 1510),\n",
      "            (161914, 3852),\n",
      "            (161914, 3909),\n",
      "            (161915,  364),\n",
      "            (161915,  618),\n",
      "            (161914, 3143)],\n",
      "           names=['user_id_enc', 'brand_id_enc'], length=4471530), Second idx len: 4065\n",
      "Brand idx len: 4064, Second idx len: 4065\n",
      "User idx len: 328994, Second idx len: 328995\n",
      "Cat idx len: 1462, Second idx len: 1463\n"
     ]
    }
   ],
   "source": [
    "# Prepare interaction matrices from the training data\n",
    "def prepare_value_matrix(train_data, test_data, user_log_path, user_filter_mode = \"ALL\", \\\n",
    "    frac_users_merchants_from_train = 0.3, normalized = True, \\\n",
    "    latent_size = 10, empty_value=-100, device=\"cuda\", val_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepares a sparse interaction matrix for the given row and column.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    np.random.seed(42)\n",
    "    #temporarily upload user_info_log\n",
    "    user_info_log = pd.read_csv(user_log_path, dtype={'time_stamp': str}).rename(columns={\"seller_id\": \"merchant_id\"})\n",
    "    #user_info_log = user_info_log[user_info_log[\"time_stamp\"] == \"1111\"]\n",
    "    user_encoder = LabelEncoder()\n",
    "    merchant_encoder = LabelEncoder()\n",
    "    brand_encoder = LabelEncoder()\n",
    "    cat_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "    # Get unique user and merchant IDs from test data\n",
    "    unique_test_user_ids = set(test_data[\"user_id\"].unique())\n",
    "    unique_test_merchant_ids = set(test_data[\"merchant_id\"].unique())\n",
    "\n",
    "    if user_filter_mode == \"TEST_ONLY\":\n",
    "        # Filter only using test data\n",
    "        user_info_log = user_info_log[\n",
    "            user_info_log['user_id'].isin(unique_test_user_ids) &\n",
    "            user_info_log['merchant_id'].isin(unique_test_merchant_ids)\n",
    "        ]\n",
    "        train_df_sampled_X = None\n",
    "        train_df_sampled_y = None\n",
    "        val_df_sampled_X = None\n",
    "        val_df_sampled_y = None\n",
    "    else:\n",
    "        # Get unique user and merchant IDs from train data\n",
    "        unique_train_user_merchant_ids = train_data[[\"user_id\", \"merchant_id\", \"label\"]].drop_duplicates()\n",
    "        \n",
    "        if user_filter_mode == \"TEST_ALL_TRAIN_PARTIAL\":\n",
    "            grouped = unique_train_user_merchant_ids.groupby('label')\n",
    "            sampled = pd.DataFrame()  # Initialize an empty DataFrame to hold samples\n",
    "            for name, group in grouped:\n",
    "                group_sample = group.sample(frac=frac_users_merchants_from_train, random_state=42)\n",
    "                sampled = pd.concat([sampled, group_sample], axis=0)\n",
    "            sampled = sampled.sample(frac=1, random_state=42).reset_index(drop=True) #shuffle dataframe\n",
    "            \n",
    "            # Combine user and merchant IDs\n",
    "            train_df_sampled_X = sampled.drop('label', axis=1)  # Assuming 'label' is your target variable\n",
    "            train_df_sampled_y = sampled['label']\n",
    "            \n",
    "            combined_user_ids = unique_test_user_ids.union(set(sampled[\"user_id\"]))\n",
    "            combined_merchant_ids = unique_test_merchant_ids.union(set(sampled[\"merchant_id\"]))\n",
    "        else:\n",
    "            # Combine all train and test user/merchant IDs\n",
    "            combined_user_ids = unique_test_user_ids.union(set(unique_train_user_merchant_ids[\"user_id\"]))\n",
    "            combined_merchant_ids = unique_test_merchant_ids.union(set(unique_train_user_merchant_ids[\"merchant_id\"]))\n",
    "            train_df_sampled_X = train_df.drop('label', axis=1)  # Assuming 'label' is your target variable\n",
    "            train_df_sampled_y = train_df['label']\n",
    "        \n",
    "        train_df_sampled_X, val_df_sampled_X, train_df_sampled_y, val_df_sampled_y = train_test_split(train_df_sampled_X, train_df_sampled_y, \\\n",
    "            test_size=val_size, random_state=42, stratify=train_df_sampled_y)\n",
    "\n",
    "        # Filter `user_info_log` based on combined IDs\n",
    "        user_info_log = user_info_log[\n",
    "            user_info_log['user_id'].isin(combined_user_ids) &\n",
    "            user_info_log['merchant_id'].isin(combined_merchant_ids)\n",
    "        ]\n",
    "\n",
    "    user_info_log['user_id_enc'] = user_encoder.fit_transform(user_info_log['user_id'])\n",
    "    user_info_log['merchant_id_enc'] = merchant_encoder.fit_transform(user_info_log['merchant_id'])\n",
    "    user_info_log['brand_id_enc'] = brand_encoder.fit_transform(user_info_log['brand_id'])\n",
    "    user_info_log['cat_id_enc'] = cat_encoder.fit_transform(user_info_log['cat_id'])\n",
    "\n",
    "    #construct the embedding matrix\n",
    "    print(\"User encoder length: \", len(user_encoder.classes_))\n",
    "    print(\"Merchant encoder length: \", len(merchant_encoder.classes_))\n",
    "    print(\"Brand encoder length: \", len(brand_encoder.classes_))\n",
    "    print(\"Cat encoder length: \", len(cat_encoder.classes_))\n",
    "\n",
    "    value_counts_user_merchants = user_info_log[['user_id_enc', 'merchant_id_enc']].value_counts()\n",
    "    value_counts_user_brands = user_info_log[['user_id_enc', 'brand_id_enc']].value_counts()\n",
    "    value_counts_user_cats = user_info_log[['user_id_enc', 'cat_id_enc']].value_counts()\n",
    "\n",
    "    del user_info_log\n",
    "\n",
    "    if normalized:\n",
    "        value_counts_user_merchants = (value_counts_user_merchants - value_counts_user_merchants.mean()) / value_counts_user_merchants.std()\n",
    "        value_counts_user_brands = (value_counts_user_brands - value_counts_user_brands.mean()) / value_counts_user_brands.std()\n",
    "        value_counts_user_cats = (value_counts_user_cats - value_counts_user_cats.mean()) / value_counts_user_cats.std()\n",
    "\n",
    "    #create embedding matrices for user, merchant, brand, cat\n",
    "    print(f\"User_encoder.classes_: {user_encoder.classes_}\")\n",
    "    user_embedding = torch.empty(len(user_encoder.classes_), latent_size, device=device,dtype=torch.bfloat16)\n",
    "    init.xavier_uniform_(user_embedding)\n",
    "    merchant_embedding = torch.empty(len(merchant_encoder.classes_), latent_size, device=device,dtype=torch.bfloat16)\n",
    "    init.xavier_uniform_(merchant_embedding)\n",
    "    cat_embedding = torch.empty(len(cat_encoder.classes_), latent_size, device=device,dtype=torch.bfloat16)\n",
    "    init.xavier_uniform_(cat_embedding)\n",
    "    brand_embedding = torch.empty(len(brand_encoder.classes_), latent_size, device=device,dtype=torch.bfloat16)\n",
    "    init.xavier_uniform_(brand_embedding)\n",
    "\n",
    "    #populate interaction based on value_counts\n",
    "    user_merchant_mask = torch.zeros(len(user_encoder.classes_), len(merchant_encoder.classes_), device=device,dtype=torch.bfloat16)\n",
    "    user_brand_mask = torch.zeros(len(user_encoder.classes_), len(brand_encoder.classes_), device=device,dtype=torch.bfloat16)\n",
    "    user_cat_mask = torch.zeros(len(user_encoder.classes_), len(cat_encoder.classes_), device=device,dtype=torch.bfloat16)\n",
    "    user_merchant_gt = torch.full((len(user_encoder.classes_), len(merchant_encoder.classes_)), empty_value, device=device,dtype=torch.bfloat16)\n",
    "    user_brand_gt = torch.full((len(user_encoder.classes_), len(brand_encoder.classes_)), empty_value, device=device,dtype=torch.bfloat16)\n",
    "    user_cat_gt = torch.full((len(user_encoder.classes_), len(cat_encoder.classes_)), empty_value, device=device,dtype=torch.bfloat16)\n",
    "    \n",
    "    indices = torch.tensor(value_counts_user_merchants.index.to_list(), device=device, dtype=torch.int32)\n",
    "    values = torch.tensor(value_counts_user_merchants.values, device=device, dtype=torch.bfloat16)\n",
    "    first_idx = indices[:, 0].int() #user\n",
    "    second_idx = indices[:, 1].int()  #merchant\n",
    "    print(f\"User idx len: {first_idx.max()}, Second idx len: {len(user_encoder.classes_)}\")\n",
    "    assert first_idx.max() < len(user_encoder.classes_), \"User index out of bounds\"\n",
    "    print(f\"Merchant idx len: {second_idx.max()}, Second idx len: {len(merchant_encoder.classes_)}\")\n",
    "    assert second_idx.max() < len(merchant_encoder.classes_), \"Merchant index out of bounds\"\n",
    "    user_merchant_gt[first_idx, second_idx] = values\n",
    "    user_merchant_mask[first_idx, second_idx] = 1\n",
    "    del value_counts_user_merchants\n",
    "\n",
    "    indices = torch.tensor(value_counts_user_brands.index.to_list(), device=device, dtype=torch.int32)\n",
    "    values = torch.tensor(value_counts_user_brands.values, device=device, dtype=torch.bfloat16)\n",
    "    first_idx = indices[:, 0].int() #user\n",
    "    second_idx = indices[:, 1].int() #brand\n",
    "    print(f\"User idx len: {first_idx.max()}, Second idx len: {len(user_encoder.classes_)}\")\n",
    "    assert first_idx.max() < len(user_encoder.classes_), \"User index out of bounds\"\n",
    "    print(f\"Brand idx index: {value_counts_user_brands.index}, Second idx len: {len(brand_encoder.classes_)}\")\n",
    "    print(f\"Brand idx len: {second_idx.max()}, Second idx len: {len(brand_encoder.classes_)}\")\n",
    "    assert second_idx.max() < len(brand_encoder.classes_), \"Brand index out of bounds\"\n",
    "    user_brand_mask[first_idx, second_idx] = 1\n",
    "    user_brand_gt[first_idx, second_idx] = values\n",
    "    del value_counts_user_brands\n",
    "\n",
    "    indices = torch.tensor(value_counts_user_cats.index.to_list(), device=device, dtype=torch.int32)\n",
    "    values = torch.tensor(value_counts_user_cats.values, device=device, dtype=torch.bfloat16)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    first_idx = indices[:, 0].int() #user\n",
    "    second_idx = indices[:, 1].int() #cats\n",
    "    print(f\"User idx len: {first_idx.max()}, Second idx len: {len(user_encoder.classes_)}\")\n",
    "    assert first_idx.max() < len(user_encoder.classes_), \"User index out of bounds\"\n",
    "    print(f\"Cat idx len: {second_idx.max()}, Second idx len: {len(cat_encoder.classes_)}\")\n",
    "    assert second_idx.max() < len(cat_encoder.classes_), \"Cat index out of bounds\"\n",
    "    user_cat_mask[first_idx, second_idx] = 1\n",
    "    user_cat_gt[first_idx, second_idx] = values\n",
    "    del value_counts_user_cats\n",
    "    del indices\n",
    "    del values\n",
    "\n",
    "    #check for any nan inside the data\n",
    "    assert not torch.isnan(user_merchant_gt).any(), \"user_merchant_gt contains NaN\"\n",
    "    assert not torch.isnan(user_brand_gt).any(), \"user_brand_gt contains NaN\"\n",
    "    assert not torch.isnan(user_cat_gt).any(), \"user_cat_gt contains NaN\"\n",
    "\n",
    "    # metadata:\n",
    "    metadata = {\n",
    "        'user_encoder': user_encoder,\n",
    "        'merchant_encoder': merchant_encoder,\n",
    "        'brand_encoder': brand_encoder,\n",
    "        'cat_encoder': cat_encoder,\n",
    "\n",
    "        'user_embedding': user_embedding,\n",
    "        'merchant_embedding': merchant_embedding,\n",
    "        'brand_embedding': brand_embedding,\n",
    "        'cat_embedding': cat_embedding,\n",
    "\n",
    "        'user_merchant_mask': user_merchant_mask,\n",
    "        'user_brand_mask': user_brand_mask,\n",
    "        'user_cat_mask': user_cat_mask,\n",
    "\n",
    "        'user_merchant_gt': user_merchant_gt,\n",
    "        'user_brand_gt': user_brand_gt,\n",
    "        'user_cat_gt': user_cat_gt,\n",
    "\n",
    "        'train_df_sampled_X': train_df_sampled_X,\n",
    "        'train_df_sampled_y': train_df_sampled_y,\n",
    "        'val_df_sampled_X': val_df_sampled_X, \n",
    "        'val_df_sampled_y': val_df_sampled_y,\n",
    "\n",
    "        # 'value_counts_user_merchants': value_counts_user_merchants,\n",
    "        # 'value_coutns_user_brands': value_counts_user_brands, \n",
    "        # 'value_counts_user_cats': value_counts_user_cats,\n",
    "    }\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return metadata\n",
    "\n",
    "#sampling_type=\"TEST_ONLY\"\n",
    "sampling_type=\"TEST_ALL_TRAIN_PARTIAL\"\n",
    "metadata = prepare_value_matrix(train_format1, test_format1, user_log_path = os.path.join(data_format1_path, \"user_log_format1.csv\"), \n",
    "                     user_filter_mode =sampling_type, frac_users_merchants_from_train = 0.5, normalized = True, \n",
    "                     latent_size = 20, empty_value=-100, device=\"cuda\", val_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata object\n",
    "metadata_path = \"metadata_file\"\n",
    "user_encoder_path = os.path.join(metadata_path, 'user_encoder.pkl')\n",
    "merchant_encoder_path = os.path.join(metadata_path, 'merchant_encoder.pkl')\n",
    "brand_encoder_path = os.path.join(metadata_path, 'brand_encoder.pkl')\n",
    "cat_encoder_path = os.path.join(metadata_path, 'cat_encoder.pkl')\n",
    "\n",
    "user_embedding_path = os.path.join(metadata_path, 'user_embedding.pth')\n",
    "merchant_embedding_path = os.path.join(metadata_path, 'merchant_embedding.pth')\n",
    "brand_embedding_path = os.path.join(metadata_path, 'brand_embedding.pth')\n",
    "cat_embedding_path = os.path.join(metadata_path, 'cat_embedding.pth')\n",
    "\n",
    "user_merchant_mask_path = os.path.join(metadata_path, 'user_merchant_mask.pth')\n",
    "user_brand_mask_path = os.path.join(metadata_path, 'user_brand_mask.pth')\n",
    "user_cat_mask_path = os.path.join(metadata_path, 'user_cat_mask.pth')\n",
    "\n",
    "user_merchant_gt_path = os.path.join(metadata_path, 'user_merchant_gt.pth')\n",
    "user_brand_gt_path = os.path.join(metadata_path, 'user_brand_gt.pth')\n",
    "user_cat_gt_path = os.path.join(metadata_path, 'user_cat_gt.pth')\n",
    "\n",
    "train_df_sampled_X_path = os.path.join(metadata_path, 'train_df_sampled_X.csv')\n",
    "train_df_sampled_y_path = os.path.join(metadata_path, 'train_df_sampled_y.csv')\n",
    "val_df_sampled_X_path = os.path.join(metadata_path, 'val_df_sampled_X.csv')\n",
    "val_df_sampled_y_path = os.path.join(metadata_path, 'val_df_sampled_y.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save \n",
    "with open(user_encoder_path, 'wb') as f:\n",
    "    pickle.dump(metadata['user_encoder'], f)\n",
    "with open(merchant_encoder_path, 'wb') as f:\n",
    "    pickle.dump(metadata['merchant_encoder'], f)\n",
    "with open(brand_encoder_path, 'wb') as f:\n",
    "    pickle.dump(metadata['brand_encoder'], f)\n",
    "with open(cat_encoder_path, 'wb') as f:\n",
    "    pickle.dump(metadata['cat_encoder'], f)\n",
    "\n",
    "#dump the rest to \n",
    "torch.save(metadata['user_embedding'], user_embedding_path)\n",
    "torch.save(metadata['merchant_embedding'], merchant_embedding_path)\n",
    "torch.save(metadata['brand_embedding'], brand_embedding_path)\n",
    "torch.save(metadata['cat_embedding'], cat_embedding_path)\n",
    "\n",
    "torch.save(metadata['user_merchant_mask'], user_merchant_mask_path)\n",
    "torch.save(metadata['user_brand_mask'], user_brand_mask_path)\n",
    "torch.save(metadata['user_cat_mask'], user_cat_mask_path)\n",
    "\n",
    "torch.save(metadata['user_merchant_gt'], user_merchant_gt_path)\n",
    "torch.save(metadata['user_brand_gt'], user_brand_gt_path)\n",
    "torch.save(metadata['user_cat_gt'], user_cat_gt_path)\n",
    "\n",
    "metadata['train_df_sampled_X'].to_csv(train_df_sampled_X_path, index=False)\n",
    "metadata['train_df_sampled_y'].to_csv(train_df_sampled_y_path, index=False)\n",
    "metadata['val_df_sampled_X'].to_csv(val_df_sampled_X_path, index=False)\n",
    "metadata['val_df_sampled_y'].to_csv(val_df_sampled_y_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to load tensors to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize metadata dictionary\n",
    "metadata = {}\n",
    "\n",
    "# Load LabelEncoders using pickle\n",
    "with open(user_encoder_path, 'rb') as f:\n",
    "    metadata['user_encoder'] = pickle.load(f)\n",
    "with open(merchant_encoder_path, 'rb') as f:\n",
    "    metadata['merchant_encoder'] = pickle.load(f)\n",
    "with open(brand_encoder_path, 'rb') as f:\n",
    "    metadata['brand_encoder'] = pickle.load(f)\n",
    "with open(cat_encoder_path, 'rb') as f:\n",
    "    metadata['cat_encoder'] = pickle.load(f)\n",
    "\n",
    "# Load PyTorch tensors and move them directly to the GPU (if CUDA is available)\n",
    "metadata['user_embedding'] = torch.load(user_embedding_path, map_location=device)\n",
    "metadata['merchant_embedding'] = torch.load(merchant_embedding_path, map_location=device)\n",
    "metadata['brand_embedding'] = torch.load(brand_embedding_path, map_location=device)\n",
    "metadata['cat_embedding'] = torch.load(cat_embedding_path, map_location=device)\n",
    "\n",
    "metadata['user_merchant_mask'] = torch.load(user_merchant_mask_path, map_location=device)\n",
    "metadata['user_brand_mask'] = torch.load(user_brand_mask_path, map_location=device)\n",
    "metadata['user_cat_mask'] = torch.load(user_cat_mask_path, map_location=device)\n",
    "\n",
    "metadata['user_merchant_gt'] = torch.load(user_merchant_gt_path, map_location=device)\n",
    "metadata['user_brand_gt'] = torch.load(user_brand_gt_path, map_location=device)\n",
    "metadata['user_cat_gt'] = torch.load(user_cat_gt_path, map_location=device)\n",
    "\n",
    "# Load DataFrames from CSV\n",
    "metadata['train_df_sampled_X'] = pd.read_csv(train_df_sampled_X_path)\n",
    "metadata['train_df_sampled_y'] = pd.read_csv(train_df_sampled_y_path)\n",
    "metadata['val_df_sampled_X'] = pd.read_csv(val_df_sampled_X_path)\n",
    "metadata['val_df_sampled_y'] = pd.read_csv(val_df_sampled_y_path)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3000 [00:00<17:59,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  0  Loss:  6422528.0  RMSE:  3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 52/3000 [00:08<07:24,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  50  Loss:  6291456.0  RMSE:  2.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 102/3000 [00:16<07:27,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  100  Loss:  5996544.0  RMSE:  2.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 152/3000 [00:23<07:09,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  150  Loss:  5537792.0  RMSE:  2.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 202/3000 [00:31<07:11,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  200  Loss:  5111808.0  RMSE:  2.671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 252/3000 [00:39<06:54,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  250  Loss:  4620288.0  RMSE:  2.546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 302/3000 [00:47<06:57,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  300  Loss:  4358144.0  RMSE:  2.46875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 352/3000 [00:54<06:39,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  350  Loss:  4292608.0  RMSE:  2.453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 402/3000 [01:02<06:41,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  400  Loss:  4259840.0  RMSE:  2.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 452/3000 [01:10<06:24,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  450  Loss:  4227072.0  RMSE:  2.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 502/3000 [01:17<06:26,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  500  Loss:  4227072.0  RMSE:  2.421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 552/3000 [01:25<06:09,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  550  Loss:  4227072.0  RMSE:  2.421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 602/3000 [01:33<06:12,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  600  Loss:  4194304.0  RMSE:  2.421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 652/3000 [01:41<05:54,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  650  Loss:  4161536.0  RMSE:  2.421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 702/3000 [01:48<05:54,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  700  Loss:  4161536.0  RMSE:  2.421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 752/3000 [01:56<05:38,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  750  Loss:  4161536.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 802/3000 [02:04<05:39,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  800  Loss:  4161536.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 852/3000 [02:11<05:23,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  850  Loss:  4161536.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 902/3000 [02:19<05:24,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  900  Loss:  4161536.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 952/3000 [02:27<05:08,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  950  Loss:  4161536.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1002/3000 [02:35<05:09,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1000  Loss:  4161536.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1052/3000 [02:42<04:53,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1050  Loss:  4128768.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1102/3000 [02:50<04:53,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1100  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1152/3000 [02:58<04:38,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1150  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1202/3000 [03:05<04:37,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1200  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 1252/3000 [03:13<04:23,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1250  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1302/3000 [03:21<04:22,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1300  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 1352/3000 [03:28<04:08,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1350  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1402/3000 [03:36<04:06,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1400  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1452/3000 [03:44<03:53,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1450  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1502/3000 [03:52<03:51,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1500  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1552/3000 [03:59<03:38,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1550  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1602/3000 [04:07<03:36,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1600  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1652/3000 [04:15<03:23,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1650  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1702/3000 [04:22<03:20,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1700  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1752/3000 [04:30<03:08,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1750  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1802/3000 [04:38<03:05,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1800  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1852/3000 [04:46<02:53,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1850  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1902/3000 [04:53<02:49,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1900  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1952/3000 [05:01<02:38,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  1950  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2002/3000 [05:09<02:34,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2000  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 2052/3000 [05:16<02:22,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2050  Loss:  4112384.0  RMSE:  2.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2102/3000 [05:24<02:18,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2100  Loss:  4112384.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 2152/3000 [05:32<02:07,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2150  Loss:  4112384.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 2202/3000 [05:40<02:03,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2200  Loss:  4112384.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 2252/3000 [05:47<01:52,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2250  Loss:  4112384.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 2302/3000 [05:55<01:47,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2300  Loss:  4112384.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 2352/3000 [06:03<01:37,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2350  Loss:  4112384.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2402/3000 [06:10<01:32,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2400  Loss:  4112384.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 2452/3000 [06:18<01:22,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2450  Loss:  4112384.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 2502/3000 [06:26<01:16,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2500  Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 2552/3000 [06:33<01:07,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2550  Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 2602/3000 [06:41<01:01,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2600  Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 2652/3000 [06:49<00:52,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2650  Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 2702/3000 [06:57<00:45,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2700  Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 2752/3000 [07:04<00:37,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2750  Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2802/3000 [07:12<00:30,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2800  Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 2852/3000 [07:20<00:22,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2850  Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2902/3000 [07:27<00:15,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2900  Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 2952/3000 [07:35<00:07,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch:  2950  Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [07:43<00:00,  6.48it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train Loss:  4096000.0  RMSE:  2.390625\n"
     ]
    }
   ],
   "source": [
    "#iterate for all versions of all_versions, which one is the best\n",
    "\n",
    "def safe_check_unique(tensor):\n",
    "    # Convert tensor to float32 if it's BFloat16\n",
    "    if tensor.dtype == torch.bfloat16:\n",
    "        tensor = tensor.to(torch.float32)\n",
    "    # Perform the unique operation\n",
    "    unique_values = torch.unique(tensor)\n",
    "    return unique_values\n",
    "\n",
    "def calculate_rmse(predictions1, actual1, mask1, predictions2, actual2, mask2, predictions3, actual3, mask3):\n",
    "    # Check inputs do not contain NaN or Inf\n",
    "    assert not torch.isnan(predictions1).any() and not torch.isinf(predictions1).any(), \"predictions1 contains NaN or Inf\"\n",
    "    assert not torch.isnan(actual1).any() and not torch.isinf(actual1).any(), \"actual1 contains NaN or Inf\"\n",
    "    assert not torch.isnan(predictions2).any() and not torch.isinf(predictions2).any(), \"predictions2 contains NaN or Inf\"\n",
    "    assert not torch.isnan(actual3).any() and not torch.isinf(actual3).any(), \"actual3 contains NaN or Inf\"\n",
    "    \n",
    "    # Calculate the squared differences\n",
    "    squared_diff1 = (predictions1 - actual1) ** 2\n",
    "    squared_diff2 = (predictions2 - actual2) ** 2\n",
    "    squared_diff3 = (predictions3 - actual3) ** 2\n",
    "    \n",
    "    # Apply the mask to the squared differences\n",
    "    squared_diff1 = squared_diff1 * mask1\n",
    "    squared_diff2 = squared_diff2 * mask2\n",
    "    squared_diff3 = squared_diff3 * mask3\n",
    "\n",
    "    # Verify masks are binary and valid\n",
    "    unique_mask1 = safe_check_unique(mask1)\n",
    "    unique_mask2 = safe_check_unique(mask2)\n",
    "    unique_mask3 = safe_check_unique(mask3)\n",
    "\n",
    "    assert unique_mask1.tolist() in [[0.0], [1.0], [0.0, 1.0]], \"mask1 contains values other than 0 and 1\"\n",
    "    assert unique_mask2.tolist() in [[0.0], [1.0], [0.0, 1.0]], \"mask2 contains values other than 0 and 1\"\n",
    "    assert unique_mask3.tolist() in [[0.0], [1.0], [0.0, 1.0]], \"mask3 contains values other than 0 and 1\"\n",
    "    \n",
    "    # Compute the mean of the squared differences where mask is 1\n",
    "    rmse1 = torch.sqrt(squared_diff1.sum() / (mask1.sum() + 1e-8))\n",
    "    rmse2 = torch.sqrt(squared_diff2.sum() / (mask2.sum() + 1e-8))\n",
    "    rmse3 = torch.sqrt(squared_diff3.sum() / (mask3.sum() + 1e-8))\n",
    "    rmse = rmse1 + rmse2 + rmse3\n",
    "\n",
    "    # Assert final RMSE values do not contain NaN\n",
    "    assert not torch.isnan(rmse).any(), \"Final rmse calculation resulted in NaN\"\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def training_iteration(epoch, lr, lambda_reg, metadata, \n",
    "                plot_graph =True, epoch_print_result = True, epoch_print_mult=50, \n",
    "                save_checkpoint = True, epoch_save_mult = 100, checkpoint_folder=\"embedding_checkpoint_folder\",\n",
    "                save_version_key = \"\"):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #initialize tensor U and V\n",
    "    UM_mask = metadata[\"user_merchant_mask\"]\n",
    "    UB_mask = metadata[\"user_brand_mask\"]\n",
    "    UC_mask = metadata[\"user_cat_mask\"]\n",
    "    UM_mask.requires_grad = False\n",
    "    UB_mask.requires_grad = False\n",
    "    UC_mask.requires_grad = False\n",
    "    UM_gt = metadata[\"user_merchant_gt\"]\n",
    "    UB_gt = metadata[\"user_brand_gt\"]\n",
    "    UC_gt = metadata[\"user_cat_gt\"]\n",
    "    UM_gt.requires_grad = False\n",
    "    UB_gt.requires_grad = False\n",
    "    UC_gt.requires_grad = False\n",
    "    \n",
    "    user_embedding = metadata[\"user_embedding\"].clone()\n",
    "    merchant_embedding = metadata[\"merchant_embedding\"].clone()\n",
    "    brand_embedding = metadata[\"brand_embedding\"].clone()\n",
    "    cat_embedding = metadata[\"cat_embedding\"].clone()\n",
    "\n",
    "    user_embedding.requires_grad = True\n",
    "    merchant_embedding.requires_grad = True\n",
    "    brand_embedding.requires_grad = True\n",
    "    cat_embedding.requires_grad = True\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = torch.optim.Adam([user_embedding, merchant_embedding, brand_embedding, cat_embedding], lr=lr)\n",
    "    loss_train_list = []\n",
    "    #loss_test_list = []\n",
    "    rmse_train_list = []\n",
    "    #rmse_test_list = []\n",
    "    counter = 0\n",
    "    for iteration in tqdm(range(epoch)):\n",
    "\n",
    "        UM_pred = user_embedding.mm(merchant_embedding.T)\n",
    "        assert not torch.isnan(UM_pred).any(), \"user_merchant_gt contains NaN\"\n",
    "        UB_pred = user_embedding.mm(brand_embedding.T)\n",
    "        assert not torch.isnan(UB_pred).any(), \"user_brand_gt contains NaN\"\n",
    "        UC_pred = user_embedding.mm(cat_embedding.T)\n",
    "        assert not torch.isnan(UC_pred).any(), \"user_cat_gt contains NaN\"\n",
    "\n",
    "        #calculate losses\n",
    "        UM_term = UM_mask*(UM_gt - UM_pred)\n",
    "        UM_term = 1/2 * torch.norm(input = UM_term, p=\"fro\")**2\n",
    "        assert not torch.isnan(UM_term).any(), \"UM_term contains NaN\"\n",
    "        UB_term = UB_mask*(UB_gt - UB_pred)\n",
    "        UB_term = 1/2 * torch.norm(input = UB_term, p=\"fro\")**2\n",
    "        assert not torch.isnan(UB_term).any(), \"UB_term contains NaN\"\n",
    "        UC_term = UC_mask*(UC_gt - UC_pred)\n",
    "        UC_term = 1/2 * torch.norm(input = UC_term, p=\"fro\")**2\n",
    "        assert not torch.isnan(UC_term).any(), \"UC_term contains NaN\"\n",
    "        error_regularization = lambda_reg * (torch.norm(input = user_embedding, p=\"fro\")**2 + torch.norm(input = merchant_embedding, p=\"fro\")**2 + \\\n",
    "            torch.norm(input = brand_embedding, p=\"fro\")**2 + torch.norm(input = cat_embedding, p=\"fro\")**2  )\n",
    "        assert not torch.isnan(error_regularization).any(), \"error_regularization contains NaN\"\n",
    "        loss_train = UM_term + UB_term+ UC_term+ error_regularization\n",
    "        loss_train_list.append(loss_train.item())\n",
    "\n",
    "        \n",
    "        #calculate rmse:\n",
    "        rmse_error_train = calculate_rmse(UM_pred, UM_gt, UM_mask, UB_pred, UB_gt, UB_mask, UC_pred, UC_gt, UC_mask).detach()\n",
    "        rmse_train_list.append(rmse_error_train.item())\n",
    "\n",
    "        #update U and V\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if epoch_print_result and not iteration % epoch_print_mult:\n",
    "            #print(\"End of epoch: \", iter, \" Loss: \", loss_train.item(), \" RMSE: \", rmse_error_train.item(), \" Test Loss: \", loss_test.item(), \" Test RMSE: \", rmse_error_test.item())\n",
    "            print(\"End of epoch: \", iteration, \" Loss: \", loss_train.item(), \" RMSE: \", rmse_error_train.item())\n",
    "        # if iter > 0 and loss_test_list[-2] - loss_test_list[-1] < test_loss_not_change_limit:\n",
    "        #     counter += 1\n",
    "        #     if counter >= test_loss_not_changing_counter:\n",
    "        #         print(\"Test loss not changing, break\")\n",
    "        #         break\n",
    "        # else:\n",
    "        #     counter = 0\n",
    "        if save_checkpoint and not iteration % epoch_save_mult:\n",
    "            user_embedding_path = os.path.join(checkpoint_folder, f\"user_embedding_{save_version_key}_{iteration}.pt\")\n",
    "            merchant_embedding_path = os.path.join(checkpoint_folder, f\"merchant_embedding_{save_version_key}_{iteration}.pt\")\n",
    "            brand_embedding_path = os.path.join(checkpoint_folder, f\"brand_embedding_{save_version_key}_{iteration}.pt\")\n",
    "            cat_embedding_path = os.path.join(checkpoint_folder, f\"cat_embedding_{save_version_key}_{iteration}.pt\")\n",
    "            torch.save(user_embedding, user_embedding_path)\n",
    "            torch.save(merchant_embedding, merchant_embedding_path)\n",
    "            torch.save(brand_embedding, brand_embedding_path)\n",
    "            torch.save(cat_embedding, cat_embedding_path)\n",
    "            \n",
    "\n",
    "    if plot_graph:\n",
    "        plt.plot(loss_train_list, label='Loss train')\n",
    "        #plt.plot(loss_test_list, label='Loss test')\n",
    "        plt.title('Loss vs. epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #plot RMSE also\n",
    "        plt.plot(rmse_train_list, label='RMSE train')\n",
    "        #plt.plot(rmse_test_list, label='RMSE test')\n",
    "        plt.title('RMSE vs. epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    #print(\"Final train Loss: \", loss_train_list[-1], \" RMSE: \", rmse_train_list[-1], \" Test Loss: \", loss_test_list[-1], \" Test RMSE: \", rmse_test_list[-1])\n",
    "    print(\"Final train Loss: \", loss_train_list[-1], \" RMSE: \", rmse_train_list[-1])\n",
    "    return user_embedding, merchant_embedding, brand_embedding, cat_embedding, loss_train_list, rmse_train_list\n",
    "\n",
    "metadata[\"user_embedding\"], metadata[\"merchant_embedding\"], \\\n",
    "    metadata[\"brand_embedding\"], metadata[\"cat_embedding\"], loss_train_list, rmse_train_list = training_iteration(epoch=3000, lr=0.001, lambda_reg=0.01, metadata=metadata, \\\n",
    "        plot_graph =True, epoch_print_result = True, epoch_print_mult=50, \\\n",
    "        save_checkpoint = True, epoch_save_mult = 100, checkpoint_folder=\"embedding_checkpoint_folder\", \\\n",
    "        save_version_key = \"version1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to load tensors to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize metadata dictionary\n",
    "metadata = {}\n",
    "\n",
    "# Load LabelEncoders using pickle\n",
    "with open(user_encoder_path, 'rb') as f:\n",
    "    metadata['user_encoder'] = pickle.load(f)\n",
    "with open(merchant_encoder_path, 'rb') as f:\n",
    "    metadata['merchant_encoder'] = pickle.load(f)\n",
    "with open(brand_encoder_path, 'rb') as f:\n",
    "    metadata['brand_encoder'] = pickle.load(f)\n",
    "with open(cat_encoder_path, 'rb') as f:\n",
    "    metadata['cat_encoder'] = pickle.load(f)\n",
    "\n",
    "# Load PyTorch tensors and move them directly to the GPU (if CUDA is available)\n",
    "metadata['user_embedding'] = torch.load(user_embedding_path, map_location=device)\n",
    "metadata['merchant_embedding'] = torch.load(merchant_embedding_path, map_location=device)\n",
    "metadata['brand_embedding'] = torch.load(brand_embedding_path, map_location=device)\n",
    "metadata['cat_embedding'] = torch.load(cat_embedding_path, map_location=device)\n",
    "\n",
    "metadata['user_merchant_mask'] = torch.load(user_merchant_mask_path, map_location=device)\n",
    "metadata['user_brand_mask'] = torch.load(user_brand_mask_path, map_location=device)\n",
    "metadata['user_cat_mask'] = torch.load(user_cat_mask_path, map_location=device)\n",
    "\n",
    "metadata['user_merchant_gt'] = torch.load(user_merchant_gt_path, map_location=device)\n",
    "metadata['user_brand_gt'] = torch.load(user_brand_gt_path, map_location=device)\n",
    "metadata['user_cat_gt'] = torch.load(user_cat_gt_path, map_location=device)\n",
    "\n",
    "# Load DataFrames from CSV\n",
    "metadata['train_df_sampled_X'] = pd.read_csv(train_df_sampled_X_path)\n",
    "metadata['train_df_sampled_y'] = pd.read_csv(train_df_sampled_y_path)\n",
    "metadata['val_df_sampled_X'] = pd.read_csv(val_df_sampled_X_path)\n",
    "metadata['val_df_sampled_y'] = pd.read_csv(val_df_sampled_y_path)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def training_iteration(metadata, chosen_versioning, grouping_method,\n",
    "    num_round, checkpoint_folder = \"embedding_checkpoint_folder\", \n",
    "    save_model_path = \"lightgbm.txt\", model=\"lightgbm\", use_embedding=True, save_model = True):\n",
    "\n",
    "    assert grouping_method in [\"unique_average\", \"average\", \"unique_sum\", \"sum\", \"mostoccurence\"]\n",
    "\n",
    "    metadata['user_embedding'] = torch.load(os.path.join(checkpoint_folder, \"user_embedding_\"+chosen_versioning), map_location=device)\n",
    "    metadata['merchant_embedding'] = torch.load(os.path.join(checkpoint_folder, \"merchant_embedding_\"+chosen_versioning), map_location=device)\n",
    "    metadata['brand_embedding'] = torch.load(os.path.join(checkpoint_folder, \"brand_embedding_\"+chosen_versioning), map_location=device)\n",
    "    metadata['cat_embedding'] = torch.load(os.path.join(checkpoint_folder, \"cat_embedding_\"+chosen_versioning), map_location=device)\n",
    "    metadata['user_embedding'].requires_grad = False\n",
    "    metadata['merchant_embedding'].requires_grad = False\n",
    "    metadata['brand_embedding'].requires_grad = False\n",
    "    metadata['cat_embedding'].requires_grad = False\n",
    "\n",
    "    def get_averaged_embedding(ids, embedding_tensor):\n",
    "        embeddings = [embedding_tensor[torch.tensor(id_list)].float().cpu() for id_list in ids]\n",
    "        aggregated_embeddings = torch.stack(embeddings).mean(0).float().cpu().numpy()\n",
    "        return aggregated_embeddings\n",
    "    def get_summed_embedding(ids, embedding_tensor):\n",
    "        embeddings = [embedding_tensor[torch.tensor(id_list)].float().cpu() for id_list in ids]\n",
    "        aggregated_embeddings = torch.stack(embeddings).sum(0).float().cpu().numpy()\n",
    "        return aggregated_embeddings\n",
    "    def get_mostoccurence_embedding(ids, embedding_tensor):\n",
    "        most_common_id, _ = Counter(ids).most_common(1)[0]\n",
    "        aggregated_embeddings = [embedding_tensor[torch.tensor(most_common_id)].float().cpu()]\n",
    "        aggregated_embeddings = torch.stack(aggregated_embeddings).float().cpu().numpy()\n",
    "        aggregated_embeddings = aggregated_embeddings.flatten()\n",
    "        return aggregated_embeddings\n",
    "    \n",
    "    chosen_embedding_method = None\n",
    "    if \"average\" in grouping_method:\n",
    "        chosen_embedding_method = get_averaged_embedding\n",
    "    elif \"sum\" in grouping_method:\n",
    "        chosen_embedding_method = get_summed_embedding\n",
    "    elif \"mostoccurence\" in grouping_method:\n",
    "        chosen_embedding_method = get_mostoccurence_embedding\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_X = metadata[\"train_df_sampled_X\"].copy()\n",
    "    train_y = metadata[\"train_df_sampled_y\"].copy()\n",
    "    valid_X = metadata[\"val_df_sampled_X\"].copy()\n",
    "    valid_y = metadata[\"val_df_sampled_y\"].copy()\n",
    "\n",
    "    # Load user_info_log and filter for specific time_stamp\n",
    "    user_info_log = pd.read_csv(user_log_format1_path, dtype={'time_stamp': str}).rename(columns={\"seller_id\": \"merchant_id\"})\n",
    "    user_info_log = user_info_log[user_info_log[\"time_stamp\"] == \"1111\"]\n",
    "    user_info_log = user_info_log.drop([\"time_stamp\", \"action_type\", \"item_id\"],axis=1)\n",
    "\n",
    "    train_X = pd.concat([train_X, train_y], axis=1)\n",
    "    train_X = train_X.merge(user_info_log, on=[\"user_id\", \"merchant_id\"], how=\"left\")\n",
    "    train_X = train_X.groupby(['user_id', 'merchant_id', \"label\"]).agg({\n",
    "        'cat_id': lambda x: list(set(x)) if (\"unique\" in grouping_method) else list(x),\n",
    "        'brand_id': lambda x: list(set(x)) if (\"unique\" in grouping_method) else list(x)\n",
    "    }).reset_index()\n",
    "    train_y = train_X[\"label\"]\n",
    "    train_X = train_X.drop(\"label\", axis=1)\n",
    "\n",
    "    train_X['encoded_user_id'] = metadata['user_encoder'].transform(train_X['user_id'])\n",
    "    train_X['encoded_merchant_id'] = metadata['merchant_encoder'].transform(train_X['merchant_id'])\n",
    "    train_X['encoded_cat_id'] = train_X['cat_id'].apply(lambda ids: [metadata['cat_encoder'].transform([id])[0] for id in ids])\n",
    "    train_X['encoded_brand_id'] = train_X['brand_id'].apply(lambda ids: [metadata['brand_encoder'].transform([id])[0] for id in ids])\n",
    "\n",
    "    # Then, fetch embeddings using the encoded indices\n",
    "    train_X['user_embeddings'] = train_X['encoded_user_id'].apply(lambda x: metadata['user_embedding'][x].float().cpu().numpy())\n",
    "    train_X['merchant_embeddings'] = train_X['encoded_merchant_id'].apply(lambda x: metadata['merchant_embedding'][x].float().cpu().numpy())\n",
    "    train_X['aggr_cat_embeddings'] = train_X['encoded_cat_id'].apply(lambda x: chosen_embedding_method(x, metadata['cat_embedding']))\n",
    "    train_X['aggr_brand_embeddings'] = train_X['encoded_brand_id'].apply(lambda x: chosen_embedding_method(x, metadata['brand_embedding']))\n",
    "\n",
    "    valid_X = pd.concat([valid_X, valid_y], axis=1)\n",
    "    valid_X = valid_X.merge(user_info_log, on=[\"user_id\", \"merchant_id\"], how=\"left\")\n",
    "    valid_X = valid_X.groupby(['user_id', 'merchant_id', \"label\"]).agg({\n",
    "        'cat_id': lambda x: list(set(x)) if (\"unique\" in grouping_method) else list(x),\n",
    "        'brand_id': lambda x: list(set(x)) if (\"unique\" in grouping_method) else list(x)\n",
    "    }).reset_index()\n",
    "    valid_y = valid_X[\"label\"]\n",
    "    valid_X = valid_X.drop(\"label\", axis=1)\n",
    "\n",
    "    valid_X['encoded_user_id'] = metadata['user_encoder'].transform(valid_X['user_id'])\n",
    "    valid_X['encoded_merchant_id'] = metadata['merchant_encoder'].transform(valid_X['merchant_id'])\n",
    "    valid_X['encoded_cat_id'] = valid_X['cat_id'].apply(lambda ids: [metadata['cat_encoder'].transform([id])[0] for id in ids])\n",
    "    valid_X['encoded_brand_id'] = valid_X['brand_id'].apply(lambda ids: [metadata['brand_encoder'].transform([id])[0] for id in ids])\n",
    "\n",
    "    # Then, fetch embeddings using the encoded indices\n",
    "    valid_X['user_embeddings'] = valid_X['encoded_user_id'].apply(lambda x: metadata['user_embedding'][x].float().cpu().numpy())\n",
    "    valid_X['merchant_embeddings'] = valid_X['encoded_merchant_id'].apply(lambda x: metadata['merchant_embedding'][x].float().cpu().numpy())\n",
    "    valid_X['aggr_cat_embeddings'] = valid_X['encoded_cat_id'].apply(lambda x: chosen_embedding_method(x, metadata['cat_embedding']))\n",
    "    valid_X['aggr_brand_embeddings'] = valid_X['encoded_brand_id'].apply(lambda x: chosen_embedding_method(x, metadata['brand_embedding']))\n",
    "    del user_info_log\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # =============== BEGIN TRAINING\n",
    "    # Define the function to expand embedding arrays into separate DataFrame columns\n",
    "    def expand_embeddings(df, column_names):\n",
    "        \"\"\"\n",
    "        Expand multiple columns containing embedded lists into multiple distinct columns for each embedding,\n",
    "        and concatenate them into a single DataFrame.\n",
    "        \n",
    "        Args:\n",
    "        df (pd.DataFrame): The original DataFrame containing embedding columns.\n",
    "        column_names (list of str): A list of column names to expand.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: A new DataFrame with expanded embedding columns.\n",
    "        \"\"\"\n",
    "        expanded_df = pd.DataFrame(index=df.index)\n",
    "        for column_name in column_names:\n",
    "            embeddings = np.stack(df[column_name].to_numpy())\n",
    "            col_names = [f'{column_name}_{i}' for i in range(embeddings.shape[1])]\n",
    "            embeddings_df = pd.DataFrame(embeddings, columns=col_names, index=df.index)\n",
    "            expanded_df = pd.concat([expanded_df, embeddings_df], axis=1)\n",
    "        return expanded_df\n",
    "\n",
    "    \n",
    "    # Configure parameters for LightGBM\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "    }\n",
    "    if use_embedding:\n",
    "        # Assuming 'metadata' contains all your embeddings and the 'train_X', 'val_X' have been prepared\n",
    "        # Expand embeddings into separate columns for training and validation sets\n",
    "        train_X = expand_embeddings(train_X, \n",
    "                                            ['user_embeddings', 'merchant_embeddings', 'aggr_cat_embeddings', 'aggr_brand_embeddings'])\n",
    "        valid_X = expand_embeddings(valid_X, \n",
    "                                        ['user_embeddings', 'merchant_embeddings', 'aggr_cat_embeddings', 'aggr_brand_embeddings'])\n",
    "\n",
    "    else:\n",
    "        train_X = metadata[\"train_df_sampled_X\"]\n",
    "        train_y = metadata[\"train_df_sampled_y\"]\n",
    "        valid_X= metadata[\"val_df_sampled_X\"]\n",
    "        valid_y = metadata[\"val_df_sampled_y\"]\n",
    "    \n",
    "    # Free up memory\n",
    "    gc.collect()\n",
    "    train_data = lgb.Dataset(train_X, label=train_y)\n",
    "    valid_data = lgb.Dataset(valid_X, label=valid_y, reference=train_data)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    lgb_model = lgb.train(params, train_data, \n",
    "        num_boost_round=num_round, valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=3)])\n",
    "\n",
    "    # Save the model\n",
    "    if save_model: \n",
    "        lgb_model.save_model(save_model_path)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_valid = lgb_model.predict(valid_X, num_iteration=lgb_model.best_iteration)\n",
    "    # Convert probabilities to binary output if needed, for example:\n",
    "    y_pred_binary_valid = (y_pred_valid >= 0.5).astype(int)\n",
    "    accuracy_valid = accuracy_score(valid_y, y_pred_binary_valid)\n",
    "    auc_valid = roc_auc_score(valid_y, y_pred_valid)\n",
    "    return lgb_model, accuracy_valid, auc_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['version1_0.pt', 'version1_100.pt', 'version1_200.pt', 'version1_300.pt', 'version1_400.pt', 'version1_500.pt', 'version1_600.pt', 'version1_700.pt', 'version1_800.pt', 'version1_900.pt', 'version1_1000.pt', 'version1_1100.pt', 'version1_1200.pt', 'version1_1300.pt', 'version1_1400.pt', 'version1_1500.pt', 'version1_1600.pt', 'version1_1700.pt', 'version1_1800.pt', 'version1_1900.pt', 'version1_2000.pt', 'version1_2100.pt', 'version1_2200.pt', 'version1_2300.pt', 'version1_2400.pt', 'version1_2500.pt', 'version1_2600.pt', 'version1_2700.pt', 'version1_2800.pt', 'version1_2900.pt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20239\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's binary_logloss: 0.222724\n",
      "version_name version1_0.pt\n",
      "validation_score:  0.6393264104663181\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19659\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_logloss: 0.222683\n",
      "version_name version1_100.pt\n",
      "validation_score:  0.6393082483078091\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20094\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's binary_logloss: 0.221998\n",
      "version_name version1_200.pt\n",
      "validation_score:  0.6441888388352258\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20053\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's binary_logloss: 0.222189\n",
      "version_name version1_300.pt\n",
      "validation_score:  0.6440435671656844\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20095\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.222674\n",
      "version_name version1_400.pt\n",
      "validation_score:  0.6474154186102352\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20112\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.222368\n",
      "version_name version1_500.pt\n",
      "validation_score:  0.6458508107311094\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20118\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.223296\n",
      "version_name version1_600.pt\n",
      "validation_score:  0.6360112725695858\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20163\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.222809\n",
      "version_name version1_700.pt\n",
      "validation_score:  0.6411761859927905\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20146\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.22202\n",
      "version_name version1_800.pt\n",
      "validation_score:  0.645688708026727\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20113\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.22311\n",
      "version_name version1_900.pt\n",
      "validation_score:  0.6359979741321714\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20114\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.223092\n",
      "version_name version1_1000.pt\n",
      "validation_score:  0.6369133520407406\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20100\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.222527\n",
      "version_name version1_1100.pt\n",
      "validation_score:  0.6425191105841227\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20116\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.222238\n",
      "version_name version1_1200.pt\n",
      "validation_score:  0.646208089443319\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20101\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.222526\n",
      "version_name version1_1300.pt\n",
      "validation_score:  0.6444261372275868\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20104\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.222589\n",
      "version_name version1_1400.pt\n",
      "validation_score:  0.6448227096865357\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20114\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.222714\n",
      "version_name version1_1500.pt\n",
      "validation_score:  0.6425941526809087\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20104\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.222526\n",
      "version_name version1_1600.pt\n",
      "validation_score:  0.6411802049623266\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20136\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.223085\n",
      "version_name version1_1700.pt\n",
      "validation_score:  0.637296613263009\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20125\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's binary_logloss: 0.222537\n",
      "version_name version1_1800.pt\n",
      "validation_score:  0.6454339258369568\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_versions)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m version_name \u001b[38;5;129;01min\u001b[39;00m all_versions:\n\u001b[0;32m---> 10\u001b[0m     model, accuracy_valid, auc_valid \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouping_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maverage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding_checkpoint_folder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_model_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlightgbm.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, version_name)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_score: \u001b[39m\u001b[38;5;124m\"\u001b[39m, auc_valid)\n",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m, in \u001b[0;36mtraining_iteration\u001b[0;34m(metadata, chosen_versioning, grouping_method, num_round, checkpoint_folder, save_model_path, model, use_embedding, save_model)\u001b[0m\n\u001b[1;32m     46\u001b[0m valid_y \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_df_sampled_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Load user_info_log and filter for specific time_stamp\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m user_info_log \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_log_format1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime_stamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseller_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerchant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     50\u001b[0m user_info_log \u001b[38;5;241m=\u001b[39m user_info_log[user_info_log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_stamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1111\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     51\u001b[0m user_info_log \u001b[38;5;241m=\u001b[39m user_info_log\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_stamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1968\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1966\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[0;32m-> 1968\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[1;32m   1976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/core/internals/construction.py:443\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[0;32m--> 443\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     missing \u001b[38;5;241m=\u001b[39m arrays\u001b[38;5;241m.\u001b[39misna()\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# GH10856\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# raise ValueError if only scalars in dict\u001b[39;00m\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/core/series.py:537\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    535\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m--> 537\u001b[0m     data, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/core/series.py:651\u001b[0m, in \u001b[0;36mSeries._init_dict\u001b[0;34m(self, data, index, dtype)\u001b[0m\n\u001b[1;32m    648\u001b[0m     keys, values \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;241m0\u001b[39m), []\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Input is now list-like, so rely on \"standard\" construction:\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# Now we just make sure the order is respected, if any\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/core/series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/core/construction.py:651\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    648\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 651\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_try_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    654\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m maybe_convert_platform(data)\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/core/construction.py:793\u001b[0m, in \u001b[0;36m_try_cast\u001b[0;34m(arr, dtype, copy)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_ndarray:\n\u001b[0;32m--> 793\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_1d_object_array_from_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m subarr\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(arr)\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1600\u001b[0m, in \u001b[0;36mconstruct_1d_object_array_from_listlike\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;66;03m# numpy will try to interpret nested lists as further dimensions, hence\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;66;03m# making a 1D array that contains list-likes is a bit tricky:\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(values), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1600\u001b[0m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages/pandas/core/series.py:1031\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1031\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   1033\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mview()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ablate and get all the versions of the embeddings\n",
    "checkpoint_folder=\"embedding_checkpoint_folder\"\n",
    "all_versions = os.listdir(checkpoint_folder)\n",
    "all_versions = [\"_\".join(i.split(\"_\")[2:]) for i in all_versions]\n",
    "all_versions = list(set(all_versions))\n",
    "# Sort the files based on the numeric value\n",
    "all_versions = sorted(all_versions, key=lambda x: int(x.split(\"_\")[1][:-3]))\n",
    "print(all_versions)\n",
    "for version_name in all_versions:\n",
    "    model, accuracy_valid, auc_valid = training_iteration(metadata, version_name, grouping_method = \"average\", num_round=1000, checkpoint_folder = \"embedding_checkpoint_folder\", \n",
    "    save_model_path = \"lightgbm.txt\", use_embedding=True, save_model = True)\n",
    "    print(\"version_name\", version_name)\n",
    "    print(\"validation_score: \", auc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18944\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.222938\n",
      "Grouping Method mostoccurence\n",
      "validation_score:  0.6389982884821452\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19964\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's binary_logloss: 0.221959\n",
      "Grouping Method unique_average\n",
      "validation_score:  0.6453381489291878\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20214\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.221851\n",
      "Grouping Method unique_sum\n",
      "validation_score:  0.6485763760362926\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20225\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.223052\n",
      "Grouping Method sum\n",
      "validation_score:  0.638473595370147\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20116\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.222238\n",
      "Grouping Method sum\n",
      "validation_score:  0.646208089443319\n"
     ]
    }
   ],
   "source": [
    "model, accuracy_valid, auc_valid = training_iteration(metadata, \"version1_1200.pt\", grouping_method = \"mostoccurence\", num_round=1000, checkpoint_folder = \"embedding_checkpoint_folder\", \n",
    "    save_model_path = \"lightgbm.txt\", use_embedding=True, save_model = True)\n",
    "print(\"Grouping Method\", \"mostoccurence\")\n",
    "print(\"validation_score: \", auc_valid)\n",
    "model, accuracy_valid, auc_valid = training_iteration(metadata, \"version1_1200.pt\", grouping_method = \"unique_average\", num_round=1000, checkpoint_folder = \"embedding_checkpoint_folder\", \n",
    "    save_model_path = \"lightgbm.txt\", use_embedding=True, save_model = True)\n",
    "print(\"Grouping Method\", \"unique_average\")\n",
    "print(\"validation_score: \", auc_valid)\n",
    "model, accuracy_valid, auc_valid = training_iteration(metadata, \"version1_1200.pt\", grouping_method = \"unique_sum\", num_round=1000, checkpoint_folder = \"embedding_checkpoint_folder\", \n",
    "    save_model_path = \"lightgbm.txt\", use_embedding=True, save_model = True)\n",
    "print(\"Grouping Method\", \"unique_sum\")\n",
    "print(\"validation_score: \", auc_valid)\n",
    "model, accuracy_valid, auc_valid = training_iteration(metadata, \"version1_1200.pt\", grouping_method = \"sum\", num_round=1000, checkpoint_folder = \"embedding_checkpoint_folder\", \n",
    "    save_model_path = \"lightgbm.txt\", use_embedding=True, save_model = True)\n",
    "print(\"Grouping Method\", \"sum\")\n",
    "print(\"validation_score: \", auc_valid)\n",
    "model, accuracy_valid, auc_valid = training_iteration(metadata, \"version1_1200.pt\", grouping_method = \"average\", num_round=1000, checkpoint_folder = \"embedding_checkpoint_folder\", \n",
    "    save_model_path = \"lightgbm.txt\", use_embedding=True, save_model = True)\n",
    "print(\"Grouping Method\", \"sum\")\n",
    "print(\"validation_score: \", auc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646208089443319\n",
      "0.9387817686970522\n"
     ]
    }
   ],
   "source": [
    "print(auc_valid)\n",
    "print(accuracy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages (from optuna) (24.2)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: tqdm in /flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages (from optuna) (4.67.0)\n",
      "Requirement already satisfied: PyYAML in /flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /flash2/aml/miniconda3/envs/wangwd24/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "Downloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 greenlet-3.1.1 optuna-4.1.0 sqlalchemy-2.0.36\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:13:14,772] A new study created in memory with name: no-name-8443874a-97f8-4901-94ea-53528daa5f56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19852\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:17:05,370] Trial 0 finished with value: 0.6293152597457451 and parameters: {'version_number': 1650, 'grouping_method': 'mostoccurence', 'num_round': 2588}. Best is trial 0 with value: 0.6293152597457451.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.223731\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19849\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:21:03,988] Trial 1 finished with value: 0.6359568500903885 and parameters: {'version_number': 950, 'grouping_method': 'mostoccurence', 'num_round': 948}. Best is trial 1 with value: 0.6359568500903885.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's binary_logloss: 0.223248\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19865\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:24:52,256] Trial 2 finished with value: 0.6331618743654764 and parameters: {'version_number': 1750, 'grouping_method': 'mostoccurence', 'num_round': 2779}. Best is trial 1 with value: 0.6359568500903885.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.223139\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19830\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:28:31,982] Trial 3 finished with value: 0.6389596091001758 and parameters: {'version_number': 1000, 'grouping_method': 'mostoccurence', 'num_round': 1375}. Best is trial 3 with value: 0.6389596091001758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_logloss: 0.222806\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19834\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:32:32,505] Trial 4 finished with value: 0.6301410811898402 and parameters: {'version_number': 1950, 'grouping_method': 'average', 'num_round': 1263}. Best is trial 3 with value: 0.6389596091001758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.223207\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19835\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:36:40,186] Trial 5 finished with value: 0.6369824552780845 and parameters: {'version_number': 1150, 'grouping_method': 'average', 'num_round': 1816}. Best is trial 3 with value: 0.6389596091001758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.22309\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19835\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:40:58,118] Trial 6 finished with value: 0.6404562528766351 and parameters: {'version_number': 1400, 'grouping_method': 'unique_average', 'num_round': 1478}. Best is trial 6 with value: 0.6404562528766351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's binary_logloss: 0.222756\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19830\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:44:53,005] Trial 7 finished with value: 0.6389596091001758 and parameters: {'version_number': 1000, 'grouping_method': 'unique_average', 'num_round': 2107}. Best is trial 6 with value: 0.6404562528766351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_logloss: 0.222806\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19863\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:48:44,543] Trial 8 finished with value: 0.6366670685636202 and parameters: {'version_number': 600, 'grouping_method': 'unique_average', 'num_round': 2086}. Best is trial 6 with value: 0.6404562528766351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.223016\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035949 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19828\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:52:09,996] Trial 9 finished with value: 0.637201809099459 and parameters: {'version_number': 550, 'grouping_method': 'unique_average', 'num_round': 1129}. Best is trial 6 with value: 0.6404562528766351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's binary_logloss: 0.223149\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20238\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:56:02,249] Trial 10 finished with value: 0.6368557809420976 and parameters: {'version_number': 0, 'grouping_method': 'unique_sum', 'num_round': 523}. Best is trial 6 with value: 0.6404562528766351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.223299\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19860\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 16:59:57,039] Trial 11 finished with value: 0.6312582779253106 and parameters: {'version_number': 1450, 'grouping_method': 'sum', 'num_round': 1548}. Best is trial 6 with value: 0.6404562528766351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.223009\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19846\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 17:03:52,444] Trial 12 finished with value: 0.6368803939306904 and parameters: {'version_number': 1350, 'grouping_method': 'unique_sum', 'num_round': 1439}. Best is trial 6 with value: 0.6404562528766351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's binary_logloss: 0.222933\n",
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19830\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 17:07:37,206] Trial 13 finished with value: 0.636027450841859 and parameters: {'version_number': 650, 'grouping_method': 'sum', 'num_round': 726}. Best is trial 6 with value: 0.6404562528766351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.223136\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='optuna_output.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "def trial_maximization(trial):\n",
    "    versions = trial.suggest_int(\"version_number\", 0, 2000, step = 50)\n",
    "    grouping_method = trial.suggest_categorical(\"grouping_method\", [\"unique_average\", \"average\", \"unique_sum\", \"sum\", \"mostoccurence\"])\n",
    "    num_round = trial.suggest_int(\"num_round\", 500, 3000)\n",
    "    _, _, auc_valid = training_iteration(metadata, f\"version1_{versions}.pt\", grouping_method = \"average\", num_round=1000, checkpoint_folder = \"embedding_checkpoint_folder\", \n",
    "    save_model_path = \"lightgbm.txt\", use_embedding=True, save_model = False)\n",
    "    logging.info(f'Trial {trial.number}, Params: {{versions: {versions}, grouping_method: {grouping_method}}}, num_round: {num_round}. AUC Result: {auc_valid}')\n",
    "    return auc_valid\n",
    "logging.info('Trial begins')\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(trial_maximization, n_trials=300)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "logging.info(\"Best hyperparameters:\", study.best_params)\n",
    "logging.info(\"Best AUC:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6381, number of negative: 97964\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 509\n",
      "[LightGBM] [Info] Number of data points in the train set: 104345, number of used features: 2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.061153 -> initscore=-2.731275\n",
      "[LightGBM] [Info] Start training from score -2.731275\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's binary_logloss: 0.228212\n",
      "Accuracy: 0.9388584352359413\n",
      "AUC Score: 0.571221618267522\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train lightgbm\n",
    "#split already included in metadata\n",
    "# Convert the datasets to LightGBM dataset format\n",
    "train_X = metadata[\"train_df_sampled_X\"]\n",
    "train_y = metadata[\"train_df_sampled_y\"]\n",
    "valid_X= metadata[\"val_df_sampled_X\"]\n",
    "valid_y = metadata[\"val_df_sampled_y\"]\n",
    "train_data = lgb.Dataset(metadata[\"train_df_sampled_X\"], label=metadata[\"train_df_sampled_y\"])\n",
    "valid_data = lgb.Dataset(metadata[\"val_df_sampled_X\"], label=metadata[\"val_df_sampled_y\"], reference=train_data)\n",
    "\n",
    "# Define parameters for the LightGBM model\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n",
    "    'objective': 'binary',    # Binary classification\n",
    "    'metric': 'binary_logloss'  # Logarithmic loss for binary classification\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[valid_data],\n",
    "                  num_boost_round=1000,\n",
    "                  early_stopping_rounds=100)\n",
    "\n",
    "y_pred_valid = model.predict(valid_X, num_iteration=model.best_iteration)\n",
    "# Convert probabilities to binary output if needed, for example:\n",
    "y_pred_binary_valid = (y_pred_valid >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_valid = accuracy_score(valid_y, y_pred_binary_valid)\n",
    "auc_valid = roc_auc_score(valid_y, y_pred_valid)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"AUC Score: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of user id duplicated in train and test:  0\n",
      "Ratio toward the whole test:  0.0\n",
      "Amount of merchant id duplicated in train and test:  260863\n",
      "Ratio toward the whole test:  0.9976518011144384\n"
     ]
    }
   ],
   "source": [
    "#how many users in train appear in test set\n",
    "print(\"Amount of user id duplicated in train and test: \", train_format1['user_id'].isin(test_format1['user_id']).sum())\n",
    "print(\"Ratio toward the whole test: \", train_format1['user_id'].isin(test_format1['user_id']).sum() / len(test_format1['user_id']))\n",
    "\n",
    "#how many merchants in train appear in test set\n",
    "print(\"Amount of merchant id duplicated in train and test: \", train_format1['merchant_id'].isin(test_format1['merchant_id']).sum())\n",
    "print(\"Ratio toward the whole test: \", train_format1['merchant_id'].isin(test_format1['merchant_id']).sum() / len(test_format1['merchant_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for each merchant, how many users get the label 1 (ratio)\n",
    "merchant_ratios = train_format1['label'].groupby(train_format1['merchant_id']).mean()\n",
    "\n",
    "sns.kdeplot(merchant_ratios, bw_adjust=1)  # bw_adjust adjusts the bandwidth; tweak as needed\n",
    "plt.title(\"KDE of Mean Label Ratio Per Merchant\")\n",
    "plt.xlabel(\"Mean Label Ratio\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label 0 ratio:  0.9388493621197253\n"
     ]
    }
   ],
   "source": [
    "# class balance or imbalance\n",
    "print(\"Train label 0 ratio: \", train_format1[train_format1['label'] == 0].shape[0] / len(train_format1['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_iteration_lightgbm(train_data, val_data, train_target, val_target, params):\n",
    "    lgbm_train_data = lgb.Dataset(train_data, label=train_target)\n",
    "    lgbm_val_data = lgb.Dataset(val_data, label=val_target, reference=train_data)\n",
    "    model = lgb.train(params, lgbm_train_data, num_boost_round=1000, valid_sets=[lgbm_val_data], early_stopping_rounds=50, verbose_eval=50)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lightgbm baseline\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "}\n",
    "baseline_train_data = train_format1.drop(['label'], axis=1)\n",
    "\n",
    "#preprocessing null in age range\n",
    "baseline_user_info = \n",
    "baseline_train_data = baseline_train_data.merge(user_info_format1, on='user_id', how='left')\n",
    "baseline_train_target = train_format1['label']\n",
    "\n",
    "baseline_train_data.join(other, lsuffix='_caller', rsuffix='_other')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
